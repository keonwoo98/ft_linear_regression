# ft_linear_regression

An introduction to machine learning - Simple linear regression implementation using gradient descent.

## ğŸ“‹ Project Overview

This project implements a **linear regression algorithm** from scratch to predict car prices based on mileage. The implementation uses **gradient descent** optimization without relying on high-level ML libraries.

### Key Concepts

- **Linear Hypothesis**: `price = Î¸â‚€ + Î¸â‚ Ã— mileage`
- **Gradient Descent**: Iterative optimization algorithm
- **Feature Normalization**: Data scaling for better convergence
- **Cost Function**: Mean Squared Error (MSE)

## ğŸš€ Quick Start

### Installation

```bash
# Install dependencies (optional, only needed for bonus visualization)
pip install -r requirements.txt
```

### Training the Model

```bash
python3 src/train.py
```

**Output:**
- Trains the model on [data/data.csv](data/data.csv)
- Saves parameters to `models/theta.json`
- Displays training progress and example predictions

### Making Predictions

```bash
python3 src/predict.py
```

**Interactive mode:**
- Prompts for mileage input
- Returns estimated price
- Allows multiple predictions

## ğŸ“ Project Structure

```
ft_linear_regression/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ data.csv              # Training dataset (24 samples)
â”œâ”€â”€ models/
â”‚   â””â”€â”€ theta.json            # Saved model parameters
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train.py              # Training program (gradient descent)
â”‚   â”œâ”€â”€ predict.py            # Prediction program
â”‚   â””â”€â”€ utils.py              # Helper functions
â”œâ”€â”€ bonus/                    # Bonus features (visualization, metrics)
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md
```

## ğŸ§® Algorithm Details

### Gradient Descent Implementation

**Update Rules** (from project PDF):

```
tmpÎ¸â‚€ = learningRate Ã— (1/m) Ã— Î£(estimatePrice(mileage[i]) - price[i])
tmpÎ¸â‚ = learningRate Ã— (1/m) Ã— Î£((estimatePrice(mileage[i]) - price[i]) Ã— mileage[i])
```

Where:
- **m**: Number of training examples
- **learningRate**: 0.1 (tunable)
- **Î£**: Sum over all training samples

**Key Features:**
- âœ… Simultaneous parameter updates (using temporary variables)
- âœ… Feature normalization (mean/std scaling)
- âœ… Cost function monitoring (MSE)
- âœ… No prohibited libraries (numpy.polyfit, sklearn, etc.)

### Current Model Performance

**Trained Parameters:**
- **Î¸â‚€** (intercept): 8499.60
- **Î¸â‚** (slope): -0.0214

**Interpretation:**
- Base price: ~8,500 when mileage = 0
- Price decreases by ~0.021 per km
- Negative correlation between mileage and price âœ“

**Example Predictions:**

| Mileage (km) | Estimated Price |
|--------------|-----------------|
| 50,000       | 7,427           |
| 100,000      | 6,355           |
| 150,000      | 5,282           |
| 200,000      | 4,210           |

## ğŸ“Š Dataset Information

**Source:** [data/data.csv](data/data.csv)

- **Samples:** 24
- **Features:** 1 (mileage in km)
- **Target:** Price
- **Mileage range:** 22,899 - 240,000 km
- **Price range:** 3,650 - 8,290

## ğŸ¯ Implementation Checklist

### Mandatory Part âœ…

- [x] **predict.py** - Price prediction program
  - [x] Prompts for mileage input
  - [x] Uses hypothesis: `estimatePrice = Î¸â‚€ + Î¸â‚ Ã— mileage`
  - [x] Handles untrained model (Î¸â‚€=0, Î¸â‚=0)
  - [x] Input validation

- [x] **train.py** - Model training program
  - [x] Reads dataset from CSV
  - [x] Implements gradient descent from scratch
  - [x] Uses specified formulas from PDF
  - [x] Simultaneous parameter updates
  - [x] Saves Î¸â‚€ and Î¸â‚ to file

- [x] **utils.py** - Helper functions
  - [x] Data loading and normalization
  - [x] Parameter persistence (save/load)
  - [x] Cost function calculation

### Bonus Part ğŸ

- [x] Data visualization (scatter plot)
- [x] Regression line plotting
- [x] Precision calculation (RÂ², MAE, RMSE)
- [x] Cost function convergence visualization
- [x] Residual analysis

## ğŸ› ï¸ Technical Details

### Why Feature Normalization?

**Problem:** Original data has large ranges
- Mileage: 22,899 - 240,000 (variance ~10Â¹â°)
- Price: 3,650 - 8,290 (variance ~10â¶)

**Solution:** Normalize using z-score
```python
normalized = (x - mean) / std
```

**Benefits:**
- âœ… Faster convergence
- âœ… Better numerical stability
- âœ… Learning rate easier to tune

### Hyperparameter Tuning

**Current Settings:**
- Learning Rate: 0.1
- Iterations: 1000

**How to adjust:**
Edit in [src/train.py](src/train.py):
```python
LEARNING_RATE = 0.1  # Increase for faster learning, decrease if diverging
ITERATIONS = 1000     # Increase if not converged
```

## ğŸ“– Usage Examples

### Basic Usage

```bash
# 1. Train model
$ python3 src/train.py

# 2. Make prediction
$ python3 src/predict.py
Enter mileage (km): 100000

============================================================
  ğŸ“Š ESTIMATION RESULT
============================================================
  Mileage: 100,000 km
  Estimated Price: 6,354.70
============================================================
```

### Before Training

```bash
$ python3 src/predict.py
âš ï¸  Warning: Model not trained yet!
   Using default parameters (Î¸â‚€=0, Î¸â‚=0)
   Run train.py first for accurate predictions.
```

## ğŸ”¬ Validation

**Cost Function Convergence:**
- Initial cost: 0.430367
- Final cost: 0.133513
- Converged after ~100 iterations âœ“

**Sanity Checks:**
- âœ… Negative slope (higher mileage â†’ lower price)
- âœ… Reasonable price range (4,000 - 7,500)
- âœ… No division by zero errors
- âœ… No prohibited libraries used

## ğŸ“š Learning Resources

**Concepts Covered:**
- Supervised learning fundamentals
- Linear regression theory
- Gradient descent optimization
- Feature scaling techniques
- Model evaluation metrics

**Formula Reference:**
- Hypothesis: `h(x) = Î¸â‚€ + Î¸â‚x`
- Cost: `J(Î¸) = (1/2m) Î£(h(xâ½â±â¾) - yâ½â±â¾)Â²`
- Update: `Î¸â±¼ := Î¸â±¼ - Î± Ã— âˆ‚J(Î¸)/âˆ‚Î¸â±¼`

## âš ï¸ Known Limitations

- **Extrapolation Risk:** Predictions outside training range may be unreliable
- **Single Feature:** Only considers mileage (ignores year, condition, etc.)
- **Linear Assumption:** Real-world relationships may be non-linear

## ğŸ“ Project Requirements

**From ft_linear_regression PDF:**
- âœ… Implement linear regression with gradient descent
- âœ… No numpy.polyfit or similar cheating libraries
- âœ… Use specified hypothesis function
- âœ… Use specified training formulas
- âœ… Simultaneous parameter updates

## ğŸ“ Author

42 School Project - ft_linear_regression

---

## ğŸ Bonus Features

### 1. Data Visualization

```bash
python3 bonus/visualize.py
```

**Features:**
- Scatter plot of training data
- Regression line overlay
- Example predictions highlighted
- Residual plot analysis

### 2. Precision Metrics

```bash
python3 bonus/precision.py
```

**Calculated Metrics:**
- **RÂ² Score**: 0.7330 (73.30% variance explained) âœ“
- **MAE**: 557.84 (average error)
- **RMSE**: 667.57 (typical error)
- **MAPE**: 9.65% (percentage error)

**Model Assessment**: Good fit âœ“

### 3. Training Visualization

```bash
python3 bonus/visualize_training.py
```

**Shows:**
- Cost function convergence over iterations
- Log-scale convergence plot
- Cost reduction per iteration
- Training statistics and performance

**Results:**
- Initial Cost: 0.4304
- Final Cost: 0.1335
- Reduction: 69.0% âœ“
- Converged at: ~100 iterations

---

**Experiment Ideas:**
- Try different learning rates (0.01, 0.05, 0.2)
- Adjust iteration count
- Compare normalized vs. non-normalized training
